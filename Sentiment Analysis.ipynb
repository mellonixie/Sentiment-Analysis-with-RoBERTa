{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom transformers import *\nimport tokenizers\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf = df.drop(['textID'], axis =1)\ndf.dropna(axis = 0, inplace = True)\ndf.reset_index(inplace=True, drop=True)\ndf","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                                    text  \\\n0                    I`d have responded, if I were going   \n1          Sooo SAD I will miss you here in San Diego!!!   \n2                              my boss is bullying me...   \n3                         what interview! leave me alone   \n4       Sons of ****, why couldn`t they put them on t...   \n...                                                  ...   \n27475   wish we could come see u on Denver  husband l...   \n27476   I`ve wondered about rake to.  The client has ...   \n27477   Yay good for both of you. Enjoy the break - y...   \n27478                         But it was worth it  ****.   \n27479     All this flirting going on - The ATG smiles...   \n\n                                           selected_text sentiment  \n0                    I`d have responded, if I were going   neutral  \n1                                               Sooo SAD  negative  \n2                                            bullying me  negative  \n3                                         leave me alone  negative  \n4                                          Sons of ****,  negative  \n...                                                  ...       ...  \n27475                                             d lost  negative  \n27476                                      , don`t force  negative  \n27477                          Yay good for both of you.  positive  \n27478                         But it was worth it  ****.  positive  \n27479  All this flirting going on - The ATG smiles. Y...   neutral  \n\n[27480 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27475</th>\n      <td>wish we could come see u on Denver  husband l...</td>\n      <td>d lost</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>27476</th>\n      <td>I`ve wondered about rake to.  The client has ...</td>\n      <td>, don`t force</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>27477</th>\n      <td>Yay good for both of you. Enjoy the break - y...</td>\n      <td>Yay good for both of you.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>27478</th>\n      <td>But it was worth it  ****.</td>\n      <td>But it was worth it  ****.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>27479</th>\n      <td>All this flirting going on - The ATG smiles...</td>\n      <td>All this flirting going on - The ATG smiles. Y...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>27480 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = df.shape[0]\n\ninput_ids = np.ones((length,192),dtype='int32')\nattention_mask = np.zeros((length,192),dtype='int32')\ntoken_type_ids = np.zeros((length,192),dtype='int32')\nstart_tokens = np.zeros((length,192),dtype='int32')\nend_tokens = np.zeros((length,192),dtype='int32')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = np.ones((length,192),dtype='int32')\nattention_mask = np.zeros((length,192),dtype='int32')\ntoken_type_ids = np.zeros((length,192),dtype='int32')\nstart_tokens = np.zeros((length,192),dtype='int32')\nend_tokens = np.zeros((length,192),dtype='int32')\n\nfor k in range(length):\n    \n    text1 = str(df.loc[k,'text'])\n    text2 = str(df.loc[k,'selected_text'])\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\ninput_ids_t = np.ones((test.shape[0],192),dtype='int32')\nattention_mask_t = np.zeros((test.shape[0],192),dtype='int32')\ntoken_type_ids_t = np.zeros((test.shape[0],192),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    text1 = str(test.loc[k,'text'])\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((192,), dtype=tf.int32)\n    att = tf.keras.layers.Input((192,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((192,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jac = []; \noof_start = np.zeros((input_ids.shape[0],192))\noof_end = np.zeros((input_ids.shape[0],192))\npreds_start = np.zeros((input_ids_t.shape[0],192))\npreds_end = np.zeros((input_ids_t.shape[0],192))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,df.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n       \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=3, batch_size=32, verbose=1,\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],\n                                                    verbose=1)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = df.loc[k,'text'] \n        else:\n            text1 = \" \"+\" \".join(df.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,df.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[{"output_type":"stream","text":"#########################\n### FOLD 1\n#########################\nEpoch 1/3\n687/687 [==============================] - 525s 764ms/step - loss: 2.3286 - activation_loss: 1.1293 - activation_1_loss: 1.1994 - val_loss: 1.7024 - val_activation_loss: 0.8679 - val_activation_1_loss: 0.8345\nEpoch 2/3\n687/687 [==============================] - 522s 760ms/step - loss: 1.6725 - activation_loss: 0.8539 - activation_1_loss: 0.8187 - val_loss: 1.6442 - val_activation_loss: 0.8466 - val_activation_1_loss: 0.7976\nEpoch 3/3\n687/687 [==============================] - 521s 759ms/step - loss: 1.5694 - activation_loss: 0.7889 - activation_1_loss: 0.7805 - val_loss: 1.6178 - val_activation_loss: 0.8320 - val_activation_1_loss: 0.7858\nPredicting OOF...\n172/172 [==============================] - 43s 250ms/step\nPredicting Test...\n111/111 [==============================] - 28s 248ms/step\n>>>> FOLD 1 Jaccard = 0.7069887840564903\n\n#########################\n### FOLD 2\n#########################\nEpoch 1/3\n687/687 [==============================] - 524s 763ms/step - loss: 2.2872 - activation_loss: 1.1348 - activation_1_loss: 1.1524 - val_loss: 1.6675 - val_activation_loss: 0.8518 - val_activation_1_loss: 0.8158\nEpoch 2/3\n687/687 [==============================] - 521s 759ms/step - loss: 1.6699 - activation_loss: 0.8539 - activation_1_loss: 0.8160 - val_loss: 1.6126 - val_activation_loss: 0.8254 - val_activation_1_loss: 0.7872\nEpoch 3/3\n687/687 [==============================] - 521s 759ms/step - loss: 1.5232 - activation_loss: 0.7855 - activation_1_loss: 0.7377 - val_loss: 1.6172 - val_activation_loss: 0.8388 - val_activation_1_loss: 0.7783\nPredicting OOF...\n172/172 [==============================] - 43s 248ms/step\nPredicting Test...\n111/111 [==============================] - 27s 246ms/step\n>>>> FOLD 2 Jaccard = 0.7051891641875885\n\n#########################\n### FOLD 3\n#########################\nEpoch 1/3\n687/687 [==============================] - 524s 763ms/step - loss: 2.1782 - activation_loss: 1.0761 - activation_1_loss: 1.1021 - val_loss: 1.6427 - val_activation_loss: 0.8389 - val_activation_1_loss: 0.8038\nEpoch 2/3\n687/687 [==============================] - 521s 758ms/step - loss: 1.6171 - activation_loss: 0.8379 - activation_1_loss: 0.7792 - val_loss: 1.6236 - val_activation_loss: 0.8330 - val_activation_1_loss: 0.7906\nEpoch 3/3\n 51/687 [=>............................] - ETA: 7:14 - loss: 1.5282 - activation_loss: 0.8089 - activation_1_loss: 0.7193","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OVERALL Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\n    \n    \ntest['selected_text'] = all\ntest[['textID','selected_text']]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}